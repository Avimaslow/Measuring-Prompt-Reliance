{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Measuring Prompt Reliance — Project Setup\n",
        "\n",
        "Our project studies **how much attention a language model gives** to different parts of a conversation.  \n",
        "We model this as three “voices” the system hears at once:\n",
        "\n",
        "- **( P )**: *System prompt* — instructions like “be polite” or “summarize.”  \n",
        "- **( Q )**: *User query* — the actual question or command.  \n",
        "- **( S )**: *Self-context* — the model’s own previous output.  \n",
        "\n",
        "We will measure, for each model layer, what fraction of its total attention mass  \n",
        "is directed toward **( P )**, **( Q )**, and **( S )**.\n",
        "\n",
        "Mathematically, we define normalized attention shares:\n",
        "\n",
        "$$\n",
        "\\text{PAM} = \\frac{\\sum \\text{Attn}_{P}}{\\sum \\text{Attn}_{P,Q,S}}, \\quad\n",
        "\\text{QAM} = \\frac{\\sum \\text{Attn}_{Q}}{\\sum \\text{Attn}_{P,Q,S}}, \\quad\n",
        "\\text{SAM} = \\frac{\\sum \\text{Attn}_{S}}{\\sum \\text{Attn}_{P,Q,S}}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "vBmJ3lvuvoCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 — Installing Dependencies\n",
        "\n",
        "We install the libraries needed for our analysis:\n",
        "\n",
        "- **PyTorch** for compute  \n",
        "- **Transformers** for pretrained LMs  \n",
        "- **Datasets** for loading Alpaca/FLAN/ShareGPT later  \n",
        "- **Matplotlib** for plots  \n",
        "- **Einops/TQDM** for tensor ops & progress\n",
        "\n",
        "Colab resets runtimes, so keeping this cell at the top makes the notebook reproducible.\n"
      ],
      "metadata": {
        "id": "rJaNwdz4z-fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install \"torch==2.3.1\" \"transformers==4.44.2\" \"accelerate>=0.33\" \"datasets>=2.20\" \"einops>=0.7\" \"tqdm\" \"matplotlib\"\n"
      ],
      "metadata": {
        "id": "KdBHFElPvY96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2 — Verifying the Environment\n",
        "\n",
        "We confirm versions and GPU availability to ensure attention extraction will run efficiently.\n"
      ],
      "metadata": {
        "id": "RGgzCjbm0EAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, torchaudio, transformers, datasets, numpy as np\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "print(\"torchaudio:\", torchaudio.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n"
      ],
      "metadata": {
        "id": "WVVt3RVWw5RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3 — Loading a Small Language Model\n",
        "\n",
        "We load a compact instruction-tuned model (TinyLlama 1.1B) and enable attention outputs with `output_attentions=True`,  \n",
        "so the model returns attention tensors $(L, H, T, S)$: layers, heads, target positions, source positions.\n",
        "\n",
        "If you are on CPU, we’ll fall back to a tiny model to keep things fast.\n",
        "\n"
      ],
      "metadata": {
        "id": "jzhpVyDZ0Inx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"bitsandbytes>=0.45.0\"\n"
      ],
      "metadata": {
        "id": "1rh9gFEwxmC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PRIMARY_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # GPU\n",
        "FALLBACK_MODEL = \"sshleifer/tiny-gpt2\"                 # CPU\n",
        "model_id = PRIMARY_MODEL if device==\"cuda\" else FALLBACK_MODEL\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    output_attentions=True,\n",
        "    dtype=torch.float16 if device==\"cuda\" else torch.float32,  # <-- no bnb here\n",
        ")\n",
        "model.to(device).eval()\n",
        "\n",
        "print(f\"Loaded {model_id} on {device} (no 8-bit).\")\n"
      ],
      "metadata": {
        "id": "4D0AY7iQ0g9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4 — Defining the Input Structure\n",
        "\n",
        "Each example is a triplet:\n",
        "\\[\n",
        "(P, Q, S) = (\\text{system prompt},\\, \\text{user prompt},\\, \\text{assistant prefix})\n",
        "\\]\n",
        "We will wrap these into a small data class and build a tagged string:\n",
        "`<P>…</P>`, `<Q>…</Q>`, `<A>…`.\n"
      ],
      "metadata": {
        "id": "1hGWCt4-1UGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Triplet:\n",
        "    system: str\n",
        "    user: str\n",
        "    assistant_prefix: str = \"\"\n",
        "\n",
        "def build_prompt(t: Triplet) -> str:\n",
        "    return f\"<P>{t.system}</P>\\n<Q>{t.user}</Q>\\n<A>{t.assistant_prefix}\"\n"
      ],
      "metadata": {
        "id": "vG1K0VkK1Yko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 6 — Tokenization and Segment Boundaries\n",
        "\n",
        "We convert the combined text into tokens and compute token spans:\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "\\text{Span}(P) &= [i_P^{\\text{start}},\\, i_P^{\\text{end}}] \\\\\n",
        "\\text{Span}(Q) &= [i_Q^{\\text{start}},\\, i_Q^{\\text{end}}] \\\\\n",
        "\\text{Span}(S) &= [i_S^{\\text{start}},\\, i_S^{\\text{end}}]\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "These spans let us aggregate attention by **source segment**.\n"
      ],
      "metadata": {
        "id": "SIk19Ize1bCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_segments(t: Triplet, tokenizer):\n",
        "    text = build_prompt(t)\n",
        "    enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    input_ids = enc[\"input_ids\"][0]\n",
        "\n",
        "    # Character spans of inner content\n",
        "    s = text\n",
        "    p_start = s.index(\"<P>\") + 3\n",
        "    p_end   = s.index(\"</P>\")\n",
        "    q_start = s.index(\"<Q>\") + 3\n",
        "    q_end   = s.index(\"</Q>\")\n",
        "    a_start = s.index(\"<A>\") + 3\n",
        "    a_end   = len(s)\n",
        "\n",
        "    # Map char spans -> token spans (simple heuristic via cumulative decode)\n",
        "    toks = [tokenizer.decode([tid], skip_special_tokens=True) for tid in input_ids.tolist()]\n",
        "    def char_to_token_span(char_lo, char_hi):\n",
        "        cum, start_tok, end_tok = 0, None, None\n",
        "        for i, tk in enumerate(toks):\n",
        "            nxt = cum + len(tk)\n",
        "            if start_tok is None and nxt > char_lo:\n",
        "                start_tok = i\n",
        "            if end_tok is None and nxt >= char_hi:\n",
        "                end_tok = i\n",
        "                break\n",
        "            cum = nxt\n",
        "        if start_tok is None: start_tok = 0\n",
        "        if end_tok   is None: end_tok   = len(toks)-1\n",
        "        return (start_tok, end_tok)\n",
        "\n",
        "    spans = {\n",
        "        \"P\": char_to_token_span(p_start, p_end),\n",
        "        \"Q\": char_to_token_span(q_start, q_end),\n",
        "        \"S\": char_to_token_span(a_start, a_end),\n",
        "    }\n",
        "    return enc, spans, text\n"
      ],
      "metadata": {
        "id": "7F1O5fOH1cZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 6 — Forward Pass and Attention Extraction\n",
        "\n",
        "We run a forward pass and retrieve attention tensors:\n",
        "$[\n",
        "\\text{attn}[l] \\in \\mathbb{R}^{H \\times T \\times S}\n",
        "$]\n",
        "for each layer \\( l \\), heads \\( H \\), target positions \\( T \\), and source positions \\( S \\).\n"
      ],
      "metadata": {
        "id": "rY2P3pY91gVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def forward_with_attn(input_ids):\n",
        "    out = model(input_ids=input_ids.to(device), output_attentions=True, use_cache=False)\n",
        "    # out.attentions is a tuple length L; each item is [batch, heads, T, S]\n",
        "    return out.logits, out.attentions\n"
      ],
      "metadata": {
        "id": "_mVA4sgz1hqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 7 — Computing PAM, QAM, SAM\n",
        "\n",
        "We define the normalized attention shares per layer:\n",
        "$[\n",
        "\\text{PAM}_l = \\frac{\\sum_{h,t} A_{h,t,P}}{\\sum_{h,t,s} A_{h,t,s}},\\quad\n",
        "\\text{QAM}_l = \\frac{\\sum_{h,t} A_{h,t,Q}}{\\sum_{h,t,s} A_{h,t,s}},\\quad\n",
        "\\text{SAM}_l = \\frac{\\sum_{h,t} A_{h,t,S}}{\\sum_{h,t,s} A_{h,t,s}}\n",
        "$]\n",
        "and average across layers to get overall shares.\n"
      ],
      "metadata": {
        "id": "jxB99I011leg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_mask(src_len, span, device):\n",
        "    m = torch.zeros(src_len, dtype=torch.bool, device=device)\n",
        "    s, e = span\n",
        "    m[s:(e+1)] = True\n",
        "    return m\n",
        "\n",
        "def compute_shares(attn_tuple, spans):\n",
        "    L = len(attn_tuple)\n",
        "    pam, qam, sam = [], [], []\n",
        "    for l in range(L):\n",
        "        A = attn_tuple[l][0]  # [H, T, S]  (batch=1)\n",
        "        A = torch.clamp(A, min=0.0)\n",
        "        A = A / (A.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "\n",
        "        Slen = A.shape[-1]\n",
        "        Pmask = segment_mask(Slen, spans[\"P\"], A.device)\n",
        "        Qmask = segment_mask(Slen, spans[\"Q\"], A.device)\n",
        "        Smask = segment_mask(Slen, spans[\"S\"], A.device)\n",
        "\n",
        "        P_share = A[..., Pmask].sum(dim=-1).mean().item()\n",
        "        Q_share = A[..., Qmask].sum(dim=-1).mean().item()\n",
        "        S_share = A[..., Smask].sum(dim=-1).mean().item()\n",
        "\n",
        "        pam.append(P_share); qam.append(Q_share); sam.append(S_share)\n",
        "\n",
        "    return {\n",
        "        \"PAM_per_layer\": pam,\n",
        "        \"QAM_per_layer\": qam,\n",
        "        \"SAM_per_layer\": sam,\n",
        "        \"PAM\": float(np.mean(pam)),\n",
        "        \"QAM\": float(np.mean(qam)),\n",
        "        \"SAM\": float(np.mean(sam)),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WKadvSBJ1nhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 8 — Sanity-Check Example\n",
        "\n",
        "We test the full pipeline on a simple prompt and inspect the resulting attention shares:\n",
        "$((\\text{PAM}, \\text{QAM}, \\text{SAM})$).\n"
      ],
      "metadata": {
        "id": "NnAj8JGx1sOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = Triplet(\n",
        "    system=\"You are a helpful, concise assistant. Use short bullet points.\",\n",
        "    user=\"Explain gravity to a 10-year-old.\",\n",
        "    assistant_prefix=\"\"\n",
        ")\n",
        "\n",
        "enc, spans, _ = tokenize_with_segments(sample, tokenizer)\n",
        "logits, attentions = forward_with_attn(enc[\"input_ids\"])\n",
        "shares = compute_shares(attentions, spans)\n",
        "\n",
        "print(\"PAM/QAM/SAM (avg over layers):\",\n",
        "      round(shares[\"PAM\"],3), round(shares[\"QAM\"],3), round(shares[\"SAM\"],3))\n",
        "print(\"Spans (token indices):\", spans)\n"
      ],
      "metadata": {
        "id": "9jA45iY21vC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 9 — Visualizing Attention by Layer\n",
        "\n",
        "We plot per-layer shares to see how focus shifts across depth:\n",
        "$[\n",
        "l \\mapsto (\\text{PAM}_l,\\, \\text{QAM}_l,\\, \\text{SAM}_l).\n",
        "$]\n"
      ],
      "metadata": {
        "id": "65O-gJ3_1yRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_layer_shares(shares_dict):\n",
        "    L = len(shares_dict[\"PAM_per_layer\"])\n",
        "    x = np.arange(L)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(x, shares_dict[\"PAM_per_layer\"], label=\"PAM (system)\")\n",
        "    plt.plot(x, shares_dict[\"QAM_per_layer\"], label=\"QAM (user)\")\n",
        "    plt.plot(x, shares_dict[\"SAM_per_layer\"], label=\"SAM (self)\")\n",
        "    plt.xlabel(\"Layer\"); plt.ylabel(\"Attention share\")\n",
        "    plt.title(\"Attention shares by layer\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "plot_layer_shares(shares)\n"
      ],
      "metadata": {
        "id": "0oVooPBZ1zyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "…the model has very few self-context tokens (S) (basically none yet), so that segment dominates trivially (the green line at 1.0).\n",
        "That’s expected, because the denominator (sum over P, Q, S) normalizes per segment, and if S covers nearly all positions, the attention mass collapses there."
      ],
      "metadata": {
        "id": "YhIkzJ2M28Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = Triplet(\n",
        "    system=\"You are a helpful assistant. Write short bullet points.\",\n",
        "    user=\"Explain gravity to a 10-year-old.\",\n",
        "    assistant_prefix=\"Gravity is the force that pulls things toward each other.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Srmvgj6P3E93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc, spans, _ = tokenize_with_segments(sample, tokenizer)\n",
        "_, attn = forward_with_attn(enc[\"input_ids\"])\n",
        "shares = compute_shares(attn, spans)\n",
        "plot_layer_shares(shares)\n"
      ],
      "metadata": {
        "id": "W-syZ2wq3HLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Why the Green Line Is Lower Now\n",
        "\n",
        "Think of the model like a student paying attention to three things:\n",
        "\n",
        "1.  **System prompt** $P$: what the teacher told it — “Be polite,” “Summarize,” etc.  \n",
        "2.  **User query** $Q$: the question on the test — “Explain gravity to a 10-year-old.”  \n",
        "3.  **Self-context** $S$: the model’s own notes — the answer it’s already written.\n",
        "\n",
        "---\n",
        "\n",
        "###  What Happened Before\n",
        "\n",
        "When the model had no real answer yet (the *assistant prefix* was empty),  \n",
        "it had almost nothing to look at except itself.  \n",
        "So all its attention went to its own “notes.”  \n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Total attention} = \\text{Attn}_P + \\text{Attn}_Q + \\text{Attn}_S\n",
        "$$\n",
        "\n",
        "but when $\\text{Attn}_P \\approx 0$ and $\\text{Attn}_Q \\approx 0$,  \n",
        "then $\\text{Attn}_S \\approx 1.0$.  \n",
        "That’s why the **green line (SAM)** was high before.\n",
        "\n",
        "---\n",
        "\n",
        "###  What Happened Now\n",
        "\n",
        "Now that we gave the model an actual answer prefix — real words in $S$ —  \n",
        "it started *sharing its focus* between the teacher’s instructions and the question too.  \n",
        "\n",
        "$$\n",
        "\\text{Attn}_P + \\text{Attn}_Q + \\text{Attn}_S = 1\n",
        "$$\n",
        "\n",
        "When $\\text{Attn}_P$ and $\\text{Attn}_Q$ increase,  \n",
        "$\\text{Attn}_S$ must go down.  \n",
        "\n",
        "So the green line becomes **lower** —  \n",
        "not because the model forgot about itself,  \n",
        "but because it’s finally *listening* to everyone else.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now\n",
        "The green line went down because attention got balanced.  \n",
        "That’s a **good** sign — it means the model is learning to pay attention to the prompt and question,  \n",
        "not just its own words.\n"
      ],
      "metadata": {
        "id": "BreCbpOn3bUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persist Results to Google Drive\n",
        "\n",
        "Colab resets runtimes. Mount Drive so datasets, results, and figures persist.\n"
      ],
      "metadata": {
        "id": "vC-6-dap15Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json, os, pathlib\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/prompt-reliance-attention\"\n",
        "pathlib.Path(PROJECT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(PROJECT_DIR, \"sanity_results.json\"), \"w\") as f:\n",
        "    json.dump({\"shares\": shares}, f, indent=2)\n",
        "\n",
        "print(\"Saved to:\", os.path.join(PROJECT_DIR, \"sanity_results.json\"))\n"
      ],
      "metadata": {
        "id": "bExpWaJE17Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
